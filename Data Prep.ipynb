{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "652c347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c211e9b9",
   "metadata": {},
   "source": [
    "### Clean Crypto data and save in Data/Cleaned/Crypto/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a0d182c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned BTC: 2556 rows → data/cleaned/crypto/btc.csv\n",
      "Cleaned ETH: 2556 rows → data/cleaned/crypto/eth.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.makedirs(\"data/cleaned/crypto\", exist_ok=True)\n",
    "\n",
    "def clean_file(raw_path, out_path, name):\n",
    "    \"\"\"\n",
    "    Reads raw CSV at raw_path (which has extra header lines),\n",
    "    finds the first actual date row, and writes a clean CSV\n",
    "    with exactly two columns: Date, Close.\n",
    "    \"\"\"\n",
    "    # 1) Locate first \"YYYY-MM-DD\" line\n",
    "    first = 0\n",
    "    with open(raw_path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if re.match(r'\\d{4}-\\d{2}-\\d{2}', line.split(',')[0]):\n",
    "                first = i\n",
    "                break\n",
    "\n",
    "    # 2) Read from that line onward\n",
    "    df = pd.read_csv(\n",
    "        raw_path,\n",
    "        skiprows=first,\n",
    "        names=[\"Date\", \"Close\"],\n",
    "        usecols=[0, 1],\n",
    "        parse_dates=[\"Date\"],\n",
    "        index_col=\"Date\"\n",
    "    )\n",
    "\n",
    "    # 3) Save cleaned CSV\n",
    "    df.to_csv(out_path)\n",
    "    print(f\"Cleaned {name}: {len(df)} rows → {out_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clean_file(\"data/crypto/btc.csv\", \"data/cleaned/crypto/btc.csv\", \"BTC\")\n",
    "    clean_file(\"data/crypto/eth.csv\", \"data/cleaned/crypto/eth.csv\", \"ETH\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e957fc",
   "metadata": {},
   "source": [
    "### Clean and Save Data/cleaned/industry/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46a136cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned industry indices to data/cleaned/industry/industries.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jp/82zjtkm90vgbycmw30_2kztm0000gn/T/ipykernel_34830/773625436.py:21: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 1) Ensure output folder exists\n",
    "os.makedirs(\"data/cleaned\", exist_ok=True)\n",
    "\n",
    "# 2) Read raw lines\n",
    "with open(\"data/indices/10_Industry_Portfolios_Daily.csv\", \"r\") as f:\n",
    "    raw = f.read().splitlines()\n",
    "\n",
    "# 3) Find end of value-weighted block\n",
    "stop = next(i for i, row in enumerate(raw)\n",
    "            if \"Average Equal Weighted Returns\" in row)\n",
    "val = raw[:stop]\n",
    "\n",
    "# 4) Locate the header line (starts with \",NoDur\")\n",
    "hdr_idx = next(i for i, row in enumerate(val) if row.startswith(\",NoDur\"))\n",
    "header = val[hdr_idx].lstrip(\",\").split(\",\")\n",
    "\n",
    "# 5) Data lines follow immediately after the header\n",
    "data_lines = val[hdr_idx+1:]\n",
    "\n",
    "# 6) Parse into DataFrame\n",
    "df = pd.read_csv(\n",
    "    io.StringIO(\"\\n\".join(data_lines)),\n",
    "    names=header,\n",
    "    parse_dates=[0],\n",
    "    index_col=0,\n",
    ")\n",
    "\n",
    "# 7) Coerce to numeric (turn “-99.99” into NaN)\n",
    "df = df.apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# 8) Extract & rename our four sectors, convert %→decimal\n",
    "ind = df[[\"NoDur\",\"Manuf\",\"HiTec\",\"Hlth\"]]\n",
    "ind = ind.rename(columns={\"NoDur\":\"Consumer\"}) / 100.0\n",
    "\n",
    "# 9) Build cumulative price indices (start at 1.0)\n",
    "price_ind = (1 + ind).cumprod()\n",
    "\n",
    "# 9.5) Keep only data between 2018-01-01 and 2024-12-30\n",
    "price_ind = price_ind.loc[\"2018-01-01\":\"2024-12-30\"]\n",
    "\n",
    "# 10) Add a Date column from the index for clarity\n",
    "price_ind_with_date = price_ind.copy()\n",
    "price_ind_with_date[\"Date\"] = price_ind_with_date.index\n",
    "\n",
    "# 11) Move Date to the first column\n",
    "cols = [\"Date\"] + [col for col in price_ind_with_date.columns if col != \"Date\"]\n",
    "price_ind_with_date = price_ind_with_date[cols]\n",
    "\n",
    "# 12) Save cleaned CSV\n",
    "out_path = \"data/cleaned/industry/industries.csv\"\n",
    "price_ind_with_date.to_csv(out_path, index=False)\n",
    "print(f\"Saved cleaned industry indices to {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253fa71b",
   "metadata": {},
   "source": [
    "### Checking any missing cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17feb6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consumer    0\n",
      "Manuf       0\n",
      "HiTec       0\n",
      "Hlth        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1) Load the industries file, telling pandas that the first column is the Date index\n",
    "ind = pd.read_csv(\n",
    "    \"data/cleaned/industry/industries.csv\",\n",
    "    index_col=0,          # treat first column as index\n",
    "    parse_dates=True      # parse that index as dates\n",
    ")\n",
    "\n",
    "# 2) Rename the columns to lowercase\n",
    "ind.columns = [\"Consumer\", \"Manuf\", \"HiTec\", \"Hlth\"]\n",
    "\n",
    "# 3) Check for missing data\n",
    "print(ind.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d570e3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged prices saved to data/cleaned/all_prices.csv\n",
      "Log-returns saved to data/cleaned/log_returns.csv\n"
     ]
    }
   ],
   "source": [
    "# 1) Ensure output folder\n",
    "os.makedirs(\"data/cleaned\", exist_ok=True)\n",
    "\n",
    "# 2) Load cleaned CSVs\n",
    "btc        = pd.read_csv(\"data/cleaned/crypto/btc.csv\", parse_dates=[\"Date\"], index_col=\"Date\")[\"Close\"]\n",
    "eth        = pd.read_csv(\"data/cleaned/crypto/eth.csv\", parse_dates=[\"Date\"], index_col=\"Date\")[\"Close\"]\n",
    "industries = pd.read_csv(\"data/cleaned/industry/industries.csv\", parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "\n",
    "# 3) Rename series\n",
    "btc.name = \"btc\"\n",
    "eth.name = \"eth\"\n",
    "\n",
    "# 4) Join into one DataFrame\n",
    "prices = industries.join([btc, eth], how=\"left\").ffill()\n",
    "\n",
    "# 5) Save merged prices\n",
    "prices.to_csv(\"data/cleaned/all_prices.csv\")\n",
    "print(\"Merged prices saved to data/cleaned/all_prices.csv\")\n",
    "\n",
    "# 6) Compute log-returns using numpy\n",
    "log_returns = np.log(prices / prices.shift(1))\n",
    "\n",
    "# 7) Save log-returns (log_returns is a DataFrame, so .to_csv works)\n",
    "log_returns.to_csv(\"data/cleaned/log_returns.csv\") # type: ignore\n",
    "print(\"Log-returns saved to data/cleaned/log_returns.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
